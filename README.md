# ğŸ§  AI Brain â€“ Document & Image Memory Search

A simple system to **ingest documents and images**, chunk and embed their text content, store it in a **FAISS** vector index, and perform **semantic search**.

---

## ğŸ“‚ Project Structure

```text
aibrain/
â”‚
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ memory_ingest.py        # Loads docs, chunks text, generates embeddings, saves FAISS index
â”‚
â”œâ”€â”€ search/
â”‚   â””â”€â”€ memory_search.py        # Loads FAISS index, allows interactive search
â”‚
â”œâ”€â”€ index/
â”‚   â”œâ”€â”€ faiss_index.index       # Saved FAISS vector index
â”‚   â””â”€â”€ chunks_metadata.json    # Chunk metadata (source file, chunk_id, etc.)
â”‚
â”œâ”€â”€ data/                       # Your input documents/images
â”‚   â”œâ”€â”€ *.pdf
â”‚   â”œâ”€â”€ *.docx
â”‚   â”œâ”€â”€ *.txt
â”‚   â””â”€â”€ *.jpg / *.png
â”‚
â””â”€â”€ README.md                   # This file
```text
---
## âš™ï¸ Installation
Clone the repository

git clone https://github.com/yourusername/aibrain.git
cd aibrain
Create & activate environment

conda create -n aibrain_310 python=3.10 -y
conda activate aibrain_310
Install dependencies

pip install -r requirements.txt

---

## ğŸ“¥ Ingest Data
Place your .pdf, .docx, .txt, and .jpg/.png files into the data/ folder, then run:

python ingestion/memory_ingest.py
This will:

Extract text (via PDFMiner, python-docx, direct read, or Tesseract OCR for images).

Chunk text into smaller pieces.

Generate embeddings using SentenceTransformers.

Save the FAISS index and metadata into the index/ folder.

---

##ğŸ” Search the Memory
Run:

python search/memory_search.py
Example:

Enter your query (or 'exit' to quit): AI
Top matches:
ai.docx (chunk 0), distance: 0.9128
ai.pdf (chunk 0), distance: 0.9128
sample.txt (chunk 0), distance: 0.9128

---

##ğŸ“¦ Dependencies
sentence-transformers â€“ For embeddings

faiss â€“ For vector search

pdfminer.six â€“ For PDF text extraction

python-docx â€“ For Word document parsing

Pillow â€“ For image handling

pytesseract â€“ For OCR

nltk â€“ For sentence tokenization

---


##ğŸš€ Next Steps
Improve OCR preprocessing for better accuracy on images.

Add support for more file formats.

Implement a web UI for easier searching.
---

##ğŸ“ License
MIT License â€“ feel free to use and modify.


---
